{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "In this assignment, I import data from the San Francisco development pipeline from SF Open data's API. I then explore residential development that is proposed or currently under construction in San Francisco in a blog post. At the end of the notebook, I include a link to the blog post. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the packages necessary for calling an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re as re\n",
    "import json    # library for working with JSON-formatted text strings\n",
    "import requests  # library for accessing content from web URLs\n",
    "import pprint  # library for making Python data structures readable\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SF Planning Department releases this data quarterly. Quarterly reports go back all the way to 2012. For now, I just want to take a look at the latest data, which is the second quarter of 2016. The API endpoint for this data is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q22016 = 'https://data.sfgov.org/resource/3n2r-nn4r.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I intend to eventually create longitudinal data, I define generalizable functions below in order to tranform data from the API calls into usable dataframes. The development pipeline data is messy and inconsistent. Therefore, I define a function called \"importdata\" that allows users to specify the field names for each API endpoint. \n",
    "\n",
    "The response we get after calling the API endpoint is a list of dictionaries (one for each development project). However, not all of these dictionaries has the same set of keys (i.e. some development projects are missing a \"affordable units\" field. Because of this, I define two functions called \"includekey\" and \"include_coor_key\", which create a list of entries of each field from each API endpoint. If a given development is lacking a field, I fill in the entry in the list with a nan value. This way, when we create the final set of keys for the final dictionary in order to create the dataframe, each key has the same amount of value entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importdata(quarter, field1, field2, field3, field4, field5, field6, field7, field8, field9, geogfield1, geogfield2):\n",
    "    '''\n",
    "    This function calls the SF open data API for the given \"quarter\" of development pipeline data.\n",
    "    It returns a dataframe with the data. It takes 11 fields corresponding to the keys in the API\n",
    "    endpoint that correspond for the values I am interested in\n",
    "    '''\n",
    "    \n",
    "    def includekey(field):\n",
    "        '''\n",
    "        This function creates a list of values from each API endpoint that correspond to the given field.\n",
    "        '''\n",
    "        list = []\n",
    "        for item in data: \n",
    "            if field in item.keys():\n",
    "                list.append(item[field])\n",
    "            else:\n",
    "                list.append(np.nan)\n",
    "        return list\n",
    "\n",
    "    def include_coor_key(one, two):\n",
    "        '''\n",
    "        This function creates a list of values from each API endpoint that correspond to the given geography field.\n",
    "        Because the geography fields typically are a dictionary within a dictionary, I create a separate function\n",
    "        here to handle those cases. \n",
    "        '''\n",
    "        list = []\n",
    "        for item in data: \n",
    "            if field1 in item.keys():\n",
    "                list.append(item[one][two])\n",
    "            else:\n",
    "                list.append(np.nan)\n",
    "        return list\n",
    "    \n",
    "    response = requests.get(quarter) #call the API\n",
    "    results = response.text #put API response into a string object\n",
    "    data = json.loads(results) #translate the json format of the data into a list\n",
    "    \n",
    "    #import fields\n",
    "    d = {}\n",
    "    d['lot_number'] = includekey(field1)\n",
    "    d['address'] = includekey(field2)\n",
    "    d['status'] = includekey(field3)\n",
    "    d['latest_date'] = includekey(field4)\n",
    "    d['units'] = includekey(field5)\n",
    "    d['net_units'] = includekey(field6)\n",
    "    d['affordable_units'] = includekey(field7)\n",
    "    d['net_affordable_units'] = includekey(field8)\n",
    "    d['zone'] = includekey(field9)\n",
    "    d['lat_lon'] = include_coor_key(geogfield1, geogfield2)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(d) #create the dataframe from the above dictionary\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the function, we import the quarter 2 2016 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q22016df = importdata(Q22016, 'apn', 'nameaddr', 'beststat', 'bestdate', 'units', 'unitsnet', 'aff', 'affnet', 'zoning_sim', 'location', 'coordinates')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I clean and export the data for the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean data after importing\n",
    "Q22016df['lon'] = Q22016df['lat_lon'].astype(str).str.split(',').str[0].str.strip('[')\n",
    "Q22016df['lat'] = Q22016df['lat_lon'].astype(str).str.split(',').str[1].str.strip(']')\n",
    "Q22016df['net_units'] = Q22016df['net_units'].astype(int) #convert to integer\n",
    "Q22016df['units'] = Q22016df['units'].astype(int) #convert to integer\n",
    "#filter out those observations that have no impact on residential construction (0 net units and 0 units)\n",
    "Q22016df = Q22016df[(Q22016df['units'] != 0) | (Q22016df['net_units'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Explore the data to figure out buckets to map in carto\n",
    "Q22016df[Q22016df['net_units'] > 0].describe(percentiles = [.1, .2, .3, .4, .5, .6, .7, .8, .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_path = \"Output/current_dev.csv\"\n",
    "Q22016df.to_csv(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Blog Post: https://www.ocf.berkeley.edu/~bgoggin/2016/10/09/mapping-sf-residential-development/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
